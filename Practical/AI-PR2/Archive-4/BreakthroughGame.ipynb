{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "05eb3cde",
   "metadata": {},
   "source": [
    "\n",
    "<br>\n",
    "<font>\n",
    "<div dir=ltr align=center>\n",
    "<img src=\"https://cdn.freebiesupply.com/logos/large/2x/sharif-logo-png-transparent.png\" width=150 height=150> <br>\n",
    "<font color=0F5298 size=7>\n",
    "Artificial Intelligence <br>\n",
    "<font color=2565AE size=5>\n",
    "Computer Engineering Department <br>\n",
    "Spring 2024<br>\n",
    "<font color=3C99D size=5>\n",
    "Practical Assignment 2 - Minimax Algorithm<br>\n",
    "<font color=696880 size=4>\n",
    "Sepehr Harfi\n",
    "\n",
    "\n",
    "____"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3a011cf",
   "metadata": {},
   "source": [
    "# Personal Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0d506013",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set your student number and name\n",
    "Student_number = '401106096'\n",
    "Name = 'Radin'\n",
    "Last_name = 'Shahdaei'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4de0ad2",
   "metadata": {},
   "source": [
    "# Rules"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cf03f32",
   "metadata": {},
   "source": [
    "<font color=red>\n",
    "Please run all the cells.\n",
    "\n",
    "Please refrain from making any changes to the existing files, such as `board.py`, `game.py`, etc. \n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "547f884c",
   "metadata": {},
   "source": [
    "#### **Note:** It is strongly recommended to execute this notebook on your local device instead of Google Colab due to limitations with graphics. However, if you still prefer using Google Colab, you can refer to this [link](https://vishnudsharma.medium.com/visualizing-tkinter-and-pygame-in-colab-272c5a245f8c) for assistance in using graphics with Colab."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db3fdd74",
   "metadata": {},
   "source": [
    "# Libraries & Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94534a71",
   "metadata": {},
   "source": [
    "**Dont change the below cell.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5e4bb0af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "curr_working_directory = os.getcwd()\n",
    "src_directory_path = os.path.join(curr_working_directory, \"src\")\n",
    "\n",
    "if os.path.exists(src_directory_path):\n",
    "    os.chdir(src_directory_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "65bcdc39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import game\n",
    "from player import Player\n",
    "from random_player import RandomPlayer\n",
    "from random_greedy_player import RandomGreedyPlayer\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cf190d9",
   "metadata": {
    "id": "5cf190d9"
   },
   "source": [
    "\n",
    "# Breakthrough Game: Minimax and AlphaBeta Players (100 Points)\n",
    "\n",
    "In this notebook, we will implement and compare two AI strategies, Minimax and AlphaBeta, for playing a simple version of the Breakthrough game. We aim to assess the performance of these strategies in terms of their execution time and win probability in matches against a Greedy player and against each other with varying depths.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba059eaf",
   "metadata": {},
   "source": [
    "# Breakthrough Game Rules\n",
    "\n",
    "The Breakthrough game is a two-player strategy board game played on an 6x6 grid. The objective of the game is to move your pieces from one end of the board to the other before your opponent does.\n",
    "\n",
    "## Game Setup\n",
    "\n",
    "- The board consists of a 6x6 grid with alternating dark and light squares.\n",
    "- Each player starts with 12 pieces placed on their respective sides of the board.\n",
    "- The pieces are initially arranged in two rows, with each row containing 6 pieces.\n",
    "\n",
    "## Piece Movement\n",
    "\n",
    "- Each player can only move their own pieces.\n",
    "- Pieces can move diagonally or straight forward one square at a time.\n",
    "- Pieces cannot move backward or sideways.\n",
    "- Pieces can capture their opponent's pieces by moving diagonally forward and landing on a square occupied by an opponent's piece.\n",
    "- Captured pieces are removed from the board.\n",
    "\n",
    "\n",
    "## Game End\n",
    "\n",
    "- If a player successfully moves one of their pieces to the opposite end of the board, they win the game.\n",
    "\n",
    "## Additional Rules\n",
    "\n",
    "- Players can only capture the opponent's piece if its in their diagonal forward squares and they cannot have two or more pieces of their color in the same square.\n",
    "- Players take turns moving their pieces.\n",
    "- Players must make a move on their turn, and they cannot skip their turn.\n",
    "- If a player has multiple legal moves, they can choose which piece to move."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffe681eb",
   "metadata": {
    "id": "ffe681eb"
   },
   "source": [
    "\n",
    "## Demo of the game with graphics\n",
    "\n",
    "Initially, using the cell below, you can see a demo gameplay of two Random Greedy Players to gain insights into the game. Additionally, you can explore the gameplay of other agents such as \"Player\" which plays in a complete greedy way and also \"RandomPlayer\", to further enhance your understanding of the game."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a02c41b4",
   "metadata": {},
   "source": [
    "**Note:** You can use the cell below anywhere you want to see the game with the graphics. For this purpose, you should only be careful to pass the right class of players to the `game.Game()`. Also, If you want to see the performance of your Minimax or AlphaBeta agents, you may need to add their classes as a python file to the `src` folder and import their classes in the cell below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b067d36",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4b067d36",
    "is_executing": true,
    "outputId": "c31c275e-abbe-4383-c984-80922eb2d829"
   },
   "outputs": [],
   "source": [
    "%%python3 \n",
    "# Change the above line to %%python if youre using Windows OS\n",
    "import game\n",
    "from player import Player\n",
    "from random_greedy_player import RandomGreedyPlayer\n",
    "from random_player import RandomPlayer\n",
    "\n",
    "\n",
    "random_greedy_game = game.Game(RandomGreedyPlayer, RandomPlayer)\n",
    "print(random_greedy_game.play(True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6ac79cb",
   "metadata": {
    "id": "d6ac79cb"
   },
   "source": [
    "\n",
    "## Minimax Agent (30 Points)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ee44bc1",
   "metadata": {},
   "source": [
    "To gain insights into the game, start by reading the `board.py`, `game.py`, and `player.py` files to understand the game implementation. Then, implement a minimax agent. Recall that the minimax algorithm evaluates game states and selects optimal moves. Compare the performance of the random greedy and minimax agents to gain insights."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "104c64f4",
   "metadata": {},
   "source": [
    "**Note:** To implement the Minimax agent, you should only modify the `MinimaxPlayer` class in the cell below. You can either use the Board class `get_scores` function or define your own score funcion here and use it for the evaluation of a board state."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49dd51e9",
   "metadata": {},
   "source": [
    "**Note:** Your minimax implementation should have a depth instance which determines the level to which the algorithm should be executed for each move. It controls the extent of the search tree explored by the algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3063de25",
   "metadata": {},
   "source": [
    "**Note:** Feel free to add cells if you need to, but don't erase the existing cells. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bf52f5b",
   "metadata": {},
   "source": [
    "### Implementation (25 Points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7f1a446c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MinimaxPlayer(Player):\n",
    "    def __init__(self, player_number, board, depth=3):\n",
    "        super().__init__(player_number, board)\n",
    "        self.depth = depth\n",
    "\n",
    "    def get_next_move(self):\n",
    "        return self.minimax(self.board.get_board_grid(), self.depth, True)[1]\n",
    "\n",
    "    def minimax(self, board, depth, maximizing_player):\n",
    "        if depth == 0 or self.board.is_game_over(board) != -1:\n",
    "            scores = self.board.get_scores(board)\n",
    "            return scores[self.player_number] - scores[self.opponent_number], None\n",
    "\n",
    "        if maximizing_player:\n",
    "            max_value = -float('inf')\n",
    "            best_move = None\n",
    "            for move in self.generate_moves(board, self.player_number):\n",
    "                new_board = self.apply_move(board, move, self.player_number)\n",
    "                value, _ = self.minimax(new_board, depth - 1, False)\n",
    "                if value > max_value:\n",
    "                    max_value = value\n",
    "                    best_move = move\n",
    "            return max_value, best_move\n",
    "        else:\n",
    "            min_value = float('inf')\n",
    "            best_move = None\n",
    "            for move in self.generate_moves(board, self.opponent_number):\n",
    "                new_board = self.apply_move(board, move, self.opponent_number)\n",
    "                value, _ = self.minimax(new_board, depth - 1, True)\n",
    "                if value < min_value:\n",
    "                    min_value = value\n",
    "                    best_move = move\n",
    "            return min_value, best_move\n",
    "\n",
    "    def generate_moves(self, board, player_number):\n",
    "        moves = []\n",
    "        for i in range(self.board.get_n()):\n",
    "            for j in range(self.board.get_n()):\n",
    "                if board[i][j] == player_number:\n",
    "                    for direction in self.board.get_possible_directions(player_number):\n",
    "                        move = (i, j), (i + direction[0], j + direction[1])\n",
    "                        if self.board.is_move_valid(board, move, player_number):\n",
    "                            moves.append(move)\n",
    "        return moves\n",
    "\n",
    "    def apply_move(self, board, move, player_number):\n",
    "        new_board = [row.copy() for row in board]\n",
    "        origin, destination = move\n",
    "        new_board[origin[0]][origin[1]] = -1\n",
    "        new_board[destination[0]][destination[1]] = player_number\n",
    "        return new_board\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7855c02d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: -184.0, 1: 225.0}\n"
     ]
    }
   ],
   "source": [
    "%%python3 \n",
    "# Change the above line to %%python if youre using Windows OS\n",
    "import game\n",
    "from player import Player\n",
    "from random_greedy_player import RandomGreedyPlayer\n",
    "from minimax_player import MinimaxPlayer\n",
    "\n",
    "\n",
    "random_greedy_game = game.Game(RandomGreedyPlayer, MinimaxPlayer)\n",
    "print(random_greedy_game.play(True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6adb6b4c",
   "metadata": {},
   "source": [
    "### Time Analysis (5 Points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9c148af",
   "metadata": {},
   "source": [
    "Now play the game 5 times between two random players and calculate the average execution time. Also do this for the game of a random player and a minimax player. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c27ce0cf",
   "metadata": {
    "id": "c27ce0cf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Players Average Execution Time: 0.006 seconds\n",
      "Minimax Player and Random Player Average Execution Time: 2.157 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "number_of_games = 5\n",
    "\n",
    "start_time = time.time()\n",
    "game = Game(RandomPlayer, RandomPlayer)\n",
    "results_random = game.bulk_play(number_of_games)\n",
    "end_time = time.time()\n",
    "total_time_random = end_time - start_time\n",
    "mean_time_random = total_time_random / number_of_games\n",
    "\n",
    "print(\"Random Players Average Execution Time: {:.3f} seconds\".format(mean_time_random))\n",
    "\n",
    "start_time = time.time()\n",
    "game = Game(RandomPlayer, MinimaxPlayer)\n",
    "results_minimax = game.bulk_play(number_of_games, second_player_depth=3)\n",
    "end_time = time.time()\n",
    "total_time_minimax = end_time - start_time\n",
    "mean_time_minimax = total_time_minimax / number_of_games\n",
    "\n",
    "print(\"Minimax Player and Random Player Average Execution Time: {:.3f} seconds\".format(mean_time_minimax))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f46ed00",
   "metadata": {},
   "source": [
    "## Alphabeta Agent (30 Points)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "573b1458",
   "metadata": {},
   "source": [
    "As we can see from the above code, the Minimax algorithm takes much longer time to execute. To improve the execution time, we can implement the AlphaBeta pruning algorithm. In the next cell, we will be implementing the AlphaBeta player.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "affa687f",
   "metadata": {},
   "source": [
    "### Implementation (25 Points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0e20ed6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlphaBetaPlayer(Player):\n",
    "    def __init__(self, player_number, board, depth=3):\n",
    "        super().__init__(player_number, board)\n",
    "        self.depth = depth\n",
    "\n",
    "    def get_next_move(self):\n",
    "        return self.alphabeta(self.board.get_board_grid(), self.depth, -float('inf'), float('inf'), True)[1]\n",
    "\n",
    "    def alphabeta(self, board, depth, alpha, beta, maximizing_player):\n",
    "        if depth == 0 or self.board.is_game_over(board) != -1:\n",
    "            scores = self.board.get_scores(board)\n",
    "            return scores[self.player_number] - scores[self.opponent_number], None\n",
    "\n",
    "        if maximizing_player:\n",
    "            max_value = -float('inf')\n",
    "            best_move = None\n",
    "            for move in self.generate_moves(board, self.player_number):\n",
    "                new_board = self.apply_move(board, move, self.player_number)\n",
    "                value, _ = self.alphabeta(new_board, depth - 1, alpha, beta, False)\n",
    "                if value > max_value:\n",
    "                    max_value = value\n",
    "                    best_move = move\n",
    "                alpha = max(alpha, max_value)\n",
    "                if beta <= alpha:\n",
    "                    break\n",
    "            return max_value, best_move\n",
    "        else:\n",
    "            min_value = float('inf')\n",
    "            best_move = None\n",
    "            for move in self.generate_moves(board, self.opponent_number):\n",
    "                new_board = self.apply_move(board, move, self.opponent_number)\n",
    "                value, _ = self.alphabeta(new_board, depth - 1, alpha, beta, True)\n",
    "                if value < min_value:\n",
    "                    min_value = value\n",
    "                    best_move = move\n",
    "                beta = min(beta, min_value)\n",
    "                if beta <= alpha:\n",
    "                    break\n",
    "            return min_value, best_move\n",
    "        \n",
    "    def generate_moves(self, board, player_number):\n",
    "        moves = []\n",
    "        for i in range(self.board.get_n()):\n",
    "            for j in range(self.board.get_n()):\n",
    "                if board[i][j] == player_number:\n",
    "                    for direction in self.board.get_possible_directions(player_number):\n",
    "                        move = (i, j), (i + direction[0], j + direction[1])\n",
    "                        if self.board.is_move_valid(board, move, player_number):\n",
    "                            moves.append(move)\n",
    "        return moves\n",
    "\n",
    "    def apply_move(self, board, move, player_number):\n",
    "        new_board = [row.copy() for row in board]\n",
    "        origin, destination = move\n",
    "        new_board[origin[0]][origin[1]] = -1\n",
    "        new_board[destination[0]][destination[1]] = player_number\n",
    "        return new_board"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e66d7551",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: -188.0, 1: 225.0}\n"
     ]
    }
   ],
   "source": [
    "%%python3 \n",
    "# Change the above line to %%python if youre using Windows OS\n",
    "import game\n",
    "from player import Player\n",
    "from random_greedy_player import RandomGreedyPlayer\n",
    "from alphabeta_player import AlphaBetaPlayer\n",
    "\n",
    "\n",
    "random_greedy_game = game.Game(RandomGreedyPlayer, AlphaBetaPlayer)\n",
    "print(random_greedy_game.play(True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c703a67f",
   "metadata": {},
   "source": [
    "### Time Analysis (5 Points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c83174e",
   "metadata": {},
   "source": [
    "Now play the game 5 times between the random player and an alphabeta player and calculate the average execution time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "bce1a47c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alphabeta Player and Random Player Average Execution Time: 0.296 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "number_of_games = 5 \n",
    "\n",
    "start_time = time.time()\n",
    "game = Game(RandomPlayer, AlphaBetaPlayer)\n",
    "results_alphabeta = game.bulk_play(number_of_games, second_player_depth=3)\n",
    "end_time = time.time()\n",
    "total_time_alphabeta = end_time - start_time\n",
    "mean_time_alphabeta = total_time_alphabeta / number_of_games\n",
    "\n",
    "print(\"Alphabeta Player and Random Player Average Execution Time: {:.3f} seconds\".format(mean_time_alphabeta))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "480857e5",
   "metadata": {},
   "source": [
    "##### **Note:** The alphabeta agent should be approximately 5X-10X faster than the minimax player."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6770103",
   "metadata": {
    "id": "c6770103"
   },
   "source": [
    "\n",
    "## Win Probability Analysis (35 Points)\n",
    "\n",
    "In this section, We will simulate 50 games for your AlphaBeta player against other players and analyze their win rates. Additionally, we will have AlphaBeta players with different depths play against each other. \n",
    "\n",
    "Assume you are always the second player and the black player will always start first. \n",
    "\n",
    "If the agent is implemented correctly, with a depth of two or more it should win all the mentioned agents with a win rate > 0.7.\n",
    "\n",
    "**Note:** You can use the `bulk_play` function from `game.py` for this purpose.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "fb476097",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "depths = [1, 2, 3]  # List of different second_player_depth values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0e54af5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_results_different_depths(first_player_class, depths, first_player_depth=None, n=50):\n",
    "    results = []\n",
    "    sample_game = Game(first_player_class, AlphaBetaPlayer)\n",
    "\n",
    "    for depth in depths:\n",
    "        if first_player_depth is not None:\n",
    "            result = sample_game.bulk_play(n, first_player_depth=first_player_depth, second_player_depth=depth)\n",
    "        else:\n",
    "            result = sample_game.bulk_play(n, second_player_depth=depth)\n",
    "        win_rate = result['white'] / n  # Calculate win rate\n",
    "        results.append({'Depth': depth, 'Win Rate': win_rate})\n",
    "\n",
    "    df = pd.DataFrame(results)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e5361f9",
   "metadata": {},
   "source": [
    "### Random Player"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e8139c48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Depth  Win Rate\n",
      "0      1       1.0\n",
      "1      2       1.0\n",
      "2      3       1.0\n"
     ]
    }
   ],
   "source": [
    "results = compute_results_different_depths(RandomPlayer, depths)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6ea0e7f",
   "metadata": {},
   "source": [
    "### Random Greedy Player"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "27e22180",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Depth  Win Rate\n",
      "0      1       1.0\n",
      "1      2       1.0\n",
      "2      3       1.0\n"
     ]
    }
   ],
   "source": [
    "results = compute_results_different_depths(RandomGreedyPlayer, depths)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7e46530",
   "metadata": {},
   "source": [
    "### Greedy Player"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "37408be3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Depth  Win Rate\n",
      "0      1       1.0\n",
      "1      2       1.0\n",
      "2      3       1.0\n"
     ]
    }
   ],
   "source": [
    "results = compute_results_different_depths(Player, depths)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3218a4f",
   "metadata": {},
   "source": [
    "### Alphabeta Player with depth 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "737c7374",
   "metadata": {},
   "source": [
    "**Note:** In this part each game may take up to ~45 seconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3cd374cc",
   "metadata": {
    "id": "3cd374cc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Depth  Win Rate\n",
      "0      2       1.0\n",
      "1      3       1.0\n",
      "2      4       1.0\n"
     ]
    }
   ],
   "source": [
    "results = compute_results_different_depths(AlphaBetaPlayer, [2, 3, 4], first_player_depth=2, n=30)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4873b294",
   "metadata": {
    "id": "4873b294"
   },
   "source": [
    "\n",
    "## Conclusion (5 Points)\n",
    "\n",
    "#### Based on the results of the AlphaBeta player with other players, what is your conclusion?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d7783dc",
   "metadata": {},
   "source": [
    "**Answer:** \n",
    "\n",
    "Based on the results of the AlphaBeta player with other players, the conclusion is that the AlphaBeta player, especially when configured with a depth of two or more, demonstrates superior performance compared to other players. This is indicated by its high win rate against various opponents, suggesting that the AlphaBeta player is capable of making more optimal moves and outperforming other strategies, such as random play or less sophisticated algorithms like the minimax with lower depths.\n",
    "\n",
    "Additionally, the AlphaBeta player's win rate remains consistently high across different opponent types and depths of its own. This suggests that the AlphaBeta algorithm, with appropriate configuration, can adapt well to different game scenarios and opponents, making it a strong contender in adversarial game settings.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
